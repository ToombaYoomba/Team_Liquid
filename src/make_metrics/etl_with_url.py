from pathlib import Path
import polars as pl
from collections import defaultdict
from datetime import datetime

DATA_DIR = Path("data/raw")
OUTPUT_DIR = Path("data/metrics")
OUTPUT_DIR.mkdir(exist_ok=True)

HITS_FILES = {
    "v1": "2022_yandex_metrika_hits.parquet",
    "v2": "2024_yandex_metrika_hits.parquet",
}
VISITS_FILES = {
    "v1": "2022_yandex_metrika_visits.parquet",
    "v2": "2024_yandex_metrika_visits.parquet",
}

CHUNK_SIZE = 100_000

def add_hits_count(df: pl.DataFrame) -> pl.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–ª–æ–Ω–∫—É hits_count = –¥–ª–∏–Ω–∞ watchIDs."""
    if df.is_empty():
        return df
    return (
        df.with_columns(
            pl.col("ym:s:watchIDs")
            .str.json_decode(dtype=pl.List(pl.Utf8))
            .list.eval(pl.element().cast(pl.UInt64, strict=False))
            .alias("watchids_parsed")
        )
        .with_columns(
            pl.col("watchids_parsed").list.len().alias("hits_count")
        )
    )

def compute_advanced_metrics(version: str) -> dict:
    """–ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ–¥–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    print(f"\n=== –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó {version} ===")
    
    visits_file = DATA_DIR / VISITS_FILES[version]
    hits_file = DATA_DIR / HITS_FILES[version]
    
    # –ê–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã
    metrics = defaultdict(float)
    
    # ===============================
    # 1. –ë–ê–ó–û–í–´–ï –ú–ï–¢–†–ò–ö–ò (VISITS)
    # ===============================
    print("–ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")
    visits_lf = (
        pl.scan_parquet(visits_file)
        .select([
            "ym:s:visitID", "ym:s:counterID", "ym:s:watchIDs", "ym:s:isNewUser", 
            "ym:s:date", "ym:s:dateTime", "ym:s:dateTimeUTC", "ym:s:visitDuration", "ym:s:startURL", "ym:s:endURL"
        ])
    )
    
    total_visits = 0
    sum_visit_duration = 0.0
    new_users = 0
    bounce_visits = 0
    avg_duration = 0.0
    visit_durations = []

    batches = visits_lf.collect(streaming=True).iter_slices(CHUNK_SIZE)
    for batch in batches:
        visits = batch
        chunk_size = visits.height
        total_visits += chunk_size

        # –ù–æ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
        new_users += visits.select(pl.col("ym:s:isNewUser").sum()).item()

        visits_hc = add_hits_count(visits)
        chunk_bounce = visits_hc.select(pl.col("hits_count").eq(1).sum()).item()
        bounce_visits += chunk_bounce

        # —Å—É–º–º–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤–∏–∑–∏—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        sum_visit_duration += (
            visits
            .select(pl.col("ym:s:visitDuration").cast(pl.Float64, strict=False).sum())
            .item()
        )
        
        print(f"  Visits: {total_visits:,}")

    avg_session_duration = (
        float(sum_visit_duration / total_visits) if total_visits else 0.0
    )
    
    # ===============================
    # 2. HITS & –ü–£–¢–ò –ü–û –°–ï–ô–¢–£
    # ===============================
    print(" Hits –∏ –ø—É—Ç–∏...")
    hits_lf = (
        pl.scan_parquet(hits_file)
        .select([
            "ym:pv:watchID", "ym:pv:pageViewID", "ym:pv:URL", 
            "ym:pv:dateTime", "ym:pv:clientID"
        ])
    )
    
    total_hits = 0
    unique_users = 0
    landing_pages = defaultdict(int)
    exit_pages = defaultdict(int)
    depth_1 = depth_2plus = 0
    
    batches = hits_lf.collect(streaming=True).iter_slices(CHUNK_SIZE * 5)
    for i, batch in enumerate(batches):
        hits = batch
        chunk_hits = hits.height
        total_hits += chunk_hits

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –≤ —á–∞–Ω–∫–µ (UInt64 –±–µ–∑ –∫–∞—Å—Ç–æ–≤)
        unique_users += hits["ym:pv:clientID"].n_unique()
        
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ç–æ–ø-10)
        top_pages = (
            hits.group_by("ym:pv:URL")
            .agg(total=pl.col("ym:pv:pageViewID").count())
            .sort("total", descending=True)
            .head(10)
        )
        for row in top_pages.iter_rows(named=True):
            landing_pages[row["ym:pv:URL"]] += row["total"]
        
        print(f"  Hits: {total_hits:,}")

    # ===============================
    # 3. –ì–õ–£–ë–û–ö–ò–ï –ú–ï–¢–†–ò–ö–ò
    # ===============================
    print("–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
    
    # –ö–æ–Ω–≤–µ—Ä—Å–∏—è –ø–æ –≥–ª—É–±–∏–Ω–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
    depth_1 = bounce_visits
    depth_2plus = total_visits - bounce_visits
    
    # –¢–æ–ø –ª–µ–Ω–¥–∏–Ω–≥–∏ –∏ —ç–∫–∑–∏—Ç—ã (–∏–∑ visits)
    top_landing_df = (
        visits
        .group_by(pl.col("ym:s:startURL").fill_null("unknown"))
        .agg(pl.count().alias("count"))
        .sort("count", descending=True)
        .head(1)
    )

    # —Å–∞–º URL –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
    top_landing_url = top_landing_df["ym:s:startURL"][0]

    top_exit_df = (
        visits
        .group_by(pl.col("ym:s:endURL").fill_null("unknown"))
        .agg(pl.count().alias("count"))
        .sort("count", descending=True)
        .head(1)
    )
    top_exit_url = top_exit_df["ym:s:endURL"][0]

    # —Å–ø–∏—Å–æ–∫ (url, count) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ count –ø–æ —É–±—ã–≤–∞–Ω–∏—é
    top3 = sorted(landing_pages.items(), key=lambda kv: kv[1], reverse=True)[:3]

    # —Å—É–º–º–∞ –ø–æ–∫–∞–∑–æ–≤ —Ç–æ–ø-3
    top_pages_count = sum(count for _, count in top3)

    
    metrics.update({
        # –ë–∞–∑–æ–≤—ã–µ
        "total_visits": int(total_visits),
        "total_hits": int(total_hits),
        "unique_users": int(unique_users),
        "new_users": int(new_users),
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç—ã
        "new_user_rate": float(new_users / total_visits * 100),
        "bounce_rate": float(bounce_visits / total_visits * 100) if total_visits else 0.0,
        "pages_per_visit": float(total_hits / total_visits),
        "user_engagement": float((total_visits - bounce_visits) / total_visits * 100),
        
        # –ì–ª—É–±–∏–Ω–∞
        "visits_depth_1": int(depth_1),
        "visits_depth_2plus": int(depth_2plus),
        "deep_visits_rate": float((total_visits - bounce_visits) / total_visits * 100) if total_visits else 0.0,
        
        # –¢—Ä–∞—Ñ–∏–∫
        "unique_users": int(unique_users),
        "hits_per_user": float(total_hits / unique_users) if unique_users else 0.0,
        "visits_per_user": float(total_visits / unique_users) if unique_users else 0.0,
        
        # –ö–∞—á–µ—Å—Ç–≤–æ —Ç—Ä–∞—Ñ–∏–∫–∞
        "avg_pages": float(total_hits / total_visits),
        "session_duration_sec": avg_session_duration,
        
        # –¢–æ–ø —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø—Ä–∏–º–µ—Ä–Ω–æ)
        "top_landing_page": str(top_landing_url),
        "top_exit_page": str(top_exit_url),
        "top_pages_count": int(top_pages_count),  # —Ç–æ–ø-3
    })
    
    print(f"{version}: {total_visits:,} –≤–∏–∑–∏—Ç–æ–≤ | {total_hits:,} —Ö–∏—Ç–æ–≤ | "
          f"–æ—Ç–∫–∞–∑—ã {metrics['bounce_rate']:.1f}% | –≥–ª—É–±–∏–Ω–∞ {metrics['avg_pages']:.1f}")
    
    return dict(metrics)


def is_priem_url(col: pl.Expr) -> pl.Expr:
    """–û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ URL —Å –¥–æ–º–µ–Ω–æ–º priem.mai.ru."""
    return col.str.contains(r"^https?://priem\.mai\.ru").fill_null(False)


def compute_url_metrics(version: str, top_n: int = 200) -> pl.DataFrame:
    """
    –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º URL –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏:
    url, version, page_hits, page_visits, page_bounces, bounce_rate, avg_pages_per_visit.
    """
    visits_file = DATA_DIR / VISITS_FILES[version]
    hits_file = DATA_DIR / HITS_FILES[version]

    # ---------- VISITS: –≤–∏–∑–∏—Ç—ã –ø–æ –ª–µ–Ω–¥–∏–Ω–≥—É + bounce ----------
    visits_lf = (
        pl.scan_parquet(visits_file)
        .select([
            "ym:s:visitID",
            "ym:s:watchIDs",
            "ym:s:startURL",
        ])
    )
    visits_df = visits_lf.collect(streaming=True)
    visits_hc = add_hits_count(visits_df)

    # —Ñ–∏–ª—å—Ç—Ä —Ç–æ–ª—å–∫–æ URL priem.mai.ru –¥–ª—è –ü–û–°–¢–†–ê–ù–ò–ß–ù–´–• –º–µ—Ç—Ä–∏–∫
    visits_hc = visits_hc.filter(is_priem_url(pl.col("ym:s:startURL")))


    # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ç–∞—Ä—Ç–æ–≤–æ–º—É URL
    visits_by_url = (
        visits_hc
        .group_by(pl.col("ym:s:startURL").fill_null("unknown").alias("url"))
        .agg([
            pl.count().alias("page_visits"),
            pl.col("hits_count").eq(1).sum().alias("page_bounces"),
        ])
    )

    # ---------- HITS: —Ö–∏—Ç—ã –ø–æ URL ----------
    hits_lf = (
        pl.scan_parquet(hits_file)
        .select(["ym:pv:URL", "ym:pv:pageViewID"])
    )
    hits_df = hits_lf.collect(streaming=True)

    # —Ñ–∏–ª—å—Ç—Ä —Ç–æ–ª—å–∫–æ URL priem.mai.ru –¥–ª—è –ü–û–°–¢–†–ê–ù–ò–ß–ù–´–• –º–µ—Ç—Ä–∏–∫
    hits_df = hits_df.filter(is_priem_url(pl.col("ym:pv:URL")))


    hits_by_url = (
        hits_df
        .group_by(pl.col("ym:pv:URL").fill_null("unknown").alias("url"))
        .agg(pl.count().alias("page_hits"))
    )

    # ---------- JOIN –∏ —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ ----------
    url_metrics = (
        hits_by_url
        .join(visits_by_url, on="url", how="outer")
        .with_columns([
            pl.col("page_hits").fill_null(0),
            pl.col("page_visits").fill_null(0),
            pl.col("page_bounces").fill_null(0),
        ])
        .with_columns([
            (pl.col("page_bounces") / pl.col("page_visits") * 100)
                .fill_null(0)
                .alias("bounce_rate"),
            (pl.col("page_hits") / pl.col("page_visits"))
                .fill_null(0)
                .alias("avg_pages_per_visit"),
            pl.lit(version).alias("version"),
        ])
        .sort("page_hits", descending=True)
        .head(top_n)
    )

    return url_metrics








# === –ó–ê–ü–£–°–ö ===
print("–ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó –ú–ï–¢–†–ò–ö...")
metrics_v1 = compute_advanced_metrics("v1")
metrics_v2 = compute_advanced_metrics("v2")

print("\nüìÑ –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ URL...")
url_metrics_v1 = compute_url_metrics("v1", top_n=200)
url_metrics_v2 = compute_url_metrics("v2", top_n=200)

url_metrics_v1.write_parquet(OUTPUT_DIR / "url_metrics_v1.parquet")
url_metrics_v2.write_parquet(OUTPUT_DIR / "url_metrics_v2.parquet")

print("‚úÖ url_metrics_v1.parquet –∏ url_metrics_v2.parquet —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


# –ü–û–õ–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –°–†–ê–í–ù–ï–ù–ò–Ø
key_metrics = [
    "total_visits", "total_hits", "unique_users", "new_users",
    "new_user_rate", "bounce_rate", "pages_per_visit", 
    "deep_visits_rate", "hits_per_user", "visits_per_user"
]

comparison = pl.DataFrame({
    "metric": key_metrics,
    "v1": pl.Series("v1", [metrics_v1.get(m, 0) for m in key_metrics], dtype=pl.Float64),
    "v2": pl.Series("v2", [metrics_v2.get(m, 0) for m in key_metrics], dtype=pl.Float64)
})

print("\n=== –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò v1 vs v2 ===")
print(comparison)

# –ö–†–ê–°–ò–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê
print("\n" + "="*80)
print("–ü–û–õ–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –í–ï–†–°–ò–ô –°–ê–ô–¢–ê")
print("="*80)
print("| –ú–µ—Ç—Ä–∏–∫–∞              | v1_2022        | v2_2024        | Œî       |")
print("|----------------------|----------------|----------------|---------|")
print(f"| –í–∏–∑–∏—Ç–æ–≤              | {metrics_v1['total_visits']:,}     | {metrics_v2['total_visits']:,}     | {((metrics_v2['total_visits']/metrics_v1['total_visits']-1)*100):+5.1f}% |")
print(f"| –•–∏—Ç–æ–≤                | {metrics_v1['total_hits']:,}     | {metrics_v2['total_hits']:,}     | {((metrics_v2['total_hits']/metrics_v1['total_hits']-1)*100):+5.1f}% |")
print(f"| –£–Ω–∏–∫. –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π  | {metrics_v1['unique_users']:,}  | {metrics_v2['unique_users']:,}  | {((metrics_v2['unique_users']/metrics_v1['unique_users']-1)*100):+5.1f}% |")
print(f"| –ù–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π  | {metrics_v1['new_user_rate']:.1f}% | {metrics_v2['new_user_rate']:.1f}% | {metrics_v2['new_user_rate']-metrics_v1['new_user_rate']:+4.1f} |")
print(f"| –û—Ç–∫–∞–∑—ã               | {metrics_v1['bounce_rate']:.1f}% | {metrics_v2['bounce_rate']:.1f}% | {metrics_v2['bounce_rate']-metrics_v1['bounce_rate']:+4.1f} |")
print(f"| –°—Ç—Ä–∞–Ω–∏—Ü –∑–∞ –≤–∏–∑–∏—Ç     | {metrics_v1['pages_per_visit']:.1f}  | {metrics_v2['pages_per_visit']:.1f}  | {metrics_v2['pages_per_visit']-metrics_v1['pages_per_visit']:+4.1f} |")
print(f"| –ì–ª—É–±–æ–∫–∏–µ –≤–∏–∑–∏—Ç—ã      | {metrics_v1['deep_visits_rate']:.1f}% | {metrics_v2['deep_visits_rate']:.1f}% | {metrics_v2['deep_visits_rate']-metrics_v1['deep_visits_rate']:+4.1f} |")
print("="*80)

# –°–û–•–†–ê–ù–ï–ù–ò–ï
comparison.write_parquet(OUTPUT_DIR / "advanced_metrics.parquet")
pl.DataFrame([{"version": "v1", **metrics_v1}, {"version": "v2", **metrics_v2}]).write_parquet(OUTPUT_DIR / "full_metrics.parquet")

print(f"\n‚úÖ ‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª—ã: {OUTPUT_DIR}")
print("advanced_metrics.parquet ‚Äî –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
print("full_metrics.parquet ‚Äî –ø–æ–ª–Ω—ã–π –¥–∞–º–ø")
